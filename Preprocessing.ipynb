{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c431d65-c894-4a80-b579-53b40fc7eddd",
   "metadata": {},
   "source": [
    "WHITELIST CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1275ab-2ba1-48c4-8c5a-e3308e8830b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "''' Returns a list of tokens created by tokenizing a given text'''\n",
    "\n",
    "def tokenize(line):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    \n",
    "    i=0\n",
    "    while i < len(tokens):\n",
    "        if '\\'' in tokens[i]:\n",
    "            parts = tokens[i].split(\"'\")\n",
    "            tokens.pop(i)\n",
    "            tokens.insert(i, parts[0])\n",
    "            tokens.insert(i+1, parts[1])\n",
    "        i += 1\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b8ae1c-d6c1-4afc-b811-3d9a90dc3092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "4500000\n",
      "5000000\n",
      "5500000\n",
      "6000000\n",
      "6500000\n",
      "7000000\n",
      "7500000\n",
      "8000000\n"
     ]
    }
   ],
   "source": [
    "file = open('Corpus/paisa.raw.utf8')\n",
    "file_output = open('Corpus/paisa_distribution.txt', 'w')\n",
    "\n",
    "count = {}\n",
    "\n",
    "for index, line in enumerate(file):\n",
    "    if line[0] != '#' and line[0] != '<' and line != '\\n':\n",
    "        \n",
    "        tokens = tokenize(line)\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in count:\n",
    "                count[token] = count[token] + 1\n",
    "            else:\n",
    "                count[token] = 1\n",
    "        \n",
    "    if index % 500000 == 0:\n",
    "        print(index)\n",
    "    #if index >= 100:\n",
    "        #break\n",
    "\n",
    "count = dict(sorted(count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for k, v in count.items():\n",
    "    file_output.write(f'{k}, {v}\\n')\n",
    "    \n",
    "file.close()\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9b8d0e-054d-4a51-8e1f-f40472b4f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n",
      "36000000\n",
      "37000000\n",
      "38000000\n",
      "39000000\n",
      "40000000\n",
      "41000000\n",
      "42000000\n",
      "43000000\n",
      "44000000\n",
      "45000000\n",
      "46000000\n",
      "47000000\n",
      "48000000\n",
      "49000000\n",
      "50000000\n",
      "51000000\n",
      "52000000\n",
      "53000000\n",
      "54000000\n",
      "55000000\n",
      "56000000\n",
      "57000000\n",
      "58000000\n"
     ]
    }
   ],
   "source": [
    "file = open('Corpus/new_wiki_00')\n",
    "file_output = open('Corpus/wiki_distribution.txt', 'w')\n",
    "\n",
    "count = {}\n",
    "\n",
    "for index, line in enumerate(file):\n",
    "    if line != '\\n' and '<doc' not in line:\n",
    "        tokens = tokenize(line)\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in count:\n",
    "                count[token] = count[token] + 1\n",
    "            else:\n",
    "                count[token] = 1\n",
    "        \n",
    "    if index % 1000000 == 0:\n",
    "        print(index)\n",
    "    #if index == 100:\n",
    "        #break\n",
    "\n",
    "count = dict(sorted(count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for k, v in count.items():\n",
    "    file_output.write(f'{k}, {v}\\n')\n",
    "    \n",
    "file.close()\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24fbb2df-22f2-4e31-9129-3a5391f119b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file1 = open('Corpus/paisa_distribution.txt')\n",
    "\n",
    "paisa = []\n",
    "for i, line in enumerate(file1):\n",
    "    paisa.append((line.split(', ')[0], line[:-1].split(', ')[1]))\n",
    "\n",
    "lowercase_alphabets = [chr(i) for i in range(97, 123)] + ['è', 'é', 'à', 'ù', 'ì', 'ò']\n",
    "for index in range(len(paisa)-1, -1, -1):\n",
    "    if int(paisa[index][1]) > 3:\n",
    "        flag = True\n",
    "        for char in paisa[index][0]:\n",
    "            if char not in lowercase_alphabets:\n",
    "                flag = False\n",
    "                break\n",
    "        if not flag:\n",
    "            del paisa[index]\n",
    "    else:\n",
    "        del paisa[index]\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34832bd-ca15-4876-af4f-7450494ad0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open('Corpus/wiki_distribution.txt')\n",
    "\n",
    "wiki = []\n",
    "for i, line in enumerate(file2):\n",
    "    wiki.append((line.split(', ')[0], line[:-1].split(', ')[1]))\n",
    "\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "937f86b0-ae6e-4cda-8aa7-4cbd32c7ebea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "unique = []\n",
    "for i, (wi, pi) in enumerate(paisa):\n",
    "    flag = False\n",
    "    \n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    #if int(pi) < 4:\n",
    "        #break\n",
    "            \n",
    "    for j, (wj, pj) in enumerate(wiki):\n",
    "        \n",
    "        if int(pj) < 100:\n",
    "            break\n",
    "        \n",
    "        if wi == wj:\n",
    "            del wiki[j]\n",
    "            flag = True\n",
    "            break\n",
    "    \n",
    "    if not flag:\n",
    "        unique.append(wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922dcd55-21f5-4670-966d-2a3d2a43875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321554\n"
     ]
    }
   ],
   "source": [
    "file3 = open('Corpus/word_list.txt', 'w')\n",
    "\n",
    "print(len(unique))\n",
    "i = 0\n",
    "for word, dist in paisa:\n",
    "    if word != unique[i]:\n",
    "        file3.write(f'{word}, {dist}\\n')\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f22be6dc-3e4d-41a5-8420-1b13c1a4f92f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_distribution_file(input_file_path, output_file_path, update_rate):\n",
    "    file = open(input_file_path)\n",
    "    file_output = open(output_file_path, 'w')\n",
    "\n",
    "    count = {}\n",
    "\n",
    "    for index, line in enumerate(file):        \n",
    "        tokens = tokenize(line)\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in count:\n",
    "                count[token] = count[token] + 1\n",
    "            else:\n",
    "                count[token] = 1\n",
    "\n",
    "        if index % update_rate == 0:\n",
    "            print(index)\n",
    "\n",
    "    count = dict(sorted(count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    for k, v in count.items():\n",
    "        file_output.write(f'{k}, {v}\\n')\n",
    "\n",
    "    file.close()\n",
    "    file_output.close()\n",
    "    \n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87e0a3b-27b0-4490-88c3-a04ba02e6133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "Done.\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "Done.\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Done.\n",
      "0\n",
      "250\n",
      "500\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Generating distribution for smaller Corpuses\n",
    "generate_distribution_file('Corpus/corpuscollocazioni.txt', 'Corpus/collocazioni_distribution.txt', 50000)\n",
    "generate_distribution_file('Corpus/polirematics.txt', 'Corpus/polirematics_distribution.txt', 10000)\n",
    "generate_distribution_file('Corpus/sayings_corpus.txt', 'Corpus/sayings_distribution.txt', 1000)\n",
    "generate_distribution_file('Corpus/songs_corpus.txt', 'Corpus/songs_distribution.txt', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6364bb1-bc22-4449-b8a9-8ac1a7bc8d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paisa_distribution_file = open('Corpus/paisa_distribution.txt')\n",
    "paisa_distribution = {line.split(', ')[0]: int(line[:-1].split(', ')[1]) for line in paisa_distribution_file}\n",
    "paisa_distribution_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7fbd0fb-58d4-4791-89c0-7049ab8df8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paisa_distribution_fixed_file = open('Corpus/word_list.txt')\n",
    "paisa_distribution_fixed = {line.split(', ')[0]: int(line[:-1].split(', ')[1]) for line in paisa_distribution_fixed_file}\n",
    "paisa_distribution_fixed_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09538a98-40c0-4530-a12e-ea1b721c5d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collocazioni_distribution_file = open('Corpus/collocazioni_distribution.txt')\n",
    "collocazioni_distribution = {line.split(', ')[0]: int(line[:-1].split(', ')[1]) for line in collocazioni_distribution_file}\n",
    "collocazioni_distribution_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3063efe-a93d-46ee-bd2f-04656e45d570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated words = 19513\n",
      "added words = 47\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for w, d in collocazioni_distribution.items():\n",
    "    if w in paisa_distribution:\n",
    "        paisa_distribution_fixed[w] = paisa_distribution[w] + collocazioni_distribution[w]\n",
    "        i += 1\n",
    "    else:\n",
    "        paisa_distribution_fixed[w] = collocazioni_distribution[w]\n",
    "        j += 1\n",
    "print(f'updated words = {i}\\nadded words = {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "930ecdc9-2fa2-453e-a000-8b2efab9d2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "polirematics_distribution_file = open('Corpus/polirematics_distribution.txt')\n",
    "polirematics_distribution = {line.split(', ')[0]: int(line[:-1].split(', ')[1]) for line in polirematics_distribution_file}\n",
    "polirematics_distribution_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b0f8241-0e33-4c21-a687-04575ac02cea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated words = 14770\n",
      "added words = 427\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for w, d in polirematics_distribution.items():\n",
    "    if w in paisa_distribution:\n",
    "        paisa_distribution_fixed[w] = paisa_distribution[w] + polirematics_distribution[w]\n",
    "        i += 1\n",
    "    else:\n",
    "        paisa_distribution_fixed[w] = polirematics_distribution[w]\n",
    "        j += 1\n",
    "print(f'updated words = {i}\\nadded words = {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "114fcaa4-3a0f-4272-9e71-13b74f6570fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sayings_distribution_file = open('Corpus/sayings_distribution.txt')\n",
    "sayings_distribution = {line.split(', ')[0]: int(line[:-1].split(', ')[1]) for line in sayings_distribution_file}\n",
    "sayings_distribution_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdfa6ebf-4540-4e10-97fa-bdb3afefc0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated words = 3645\n",
      "added words = 47\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for w, d in sayings_distribution.items():\n",
    "    if w in paisa_distribution:\n",
    "        paisa_distribution_fixed[w] = paisa_distribution[w] + sayings_distribution[w]\n",
    "        i += 1\n",
    "    else:\n",
    "        paisa_distribution_fixed[w] = sayings_distribution[w]\n",
    "        j += 1\n",
    "print(f'updated words = {i}\\nadded words = {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12748344-ce62-45bb-ab44-f37e910298a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "songs_distribution_file = open('Corpus/songs_distribution.txt')\n",
    "songs_distribution = {line.split(', ')[0]: int(line[:-1].split(', ')[1]) for line in songs_distribution_file}\n",
    "songs_distribution_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0142ecb6-4a55-4349-90d5-5f2c70c19bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated words = 1359\n",
      "added words = 17\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for w, d in songs_distribution.items():\n",
    "    if w in paisa_distribution:\n",
    "        paisa_distribution_fixed[w] = paisa_distribution[w] + songs_distribution[w]\n",
    "        i += 1\n",
    "    else:\n",
    "        paisa_distribution_fixed[w] = songs_distribution[w]\n",
    "        j += 1\n",
    "print(f'updated words = {i}\\nadded words = {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fb66c8c-d7a6-4c56-b054-82da33e6f80b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "file_output = open('Corpus/whitelist.txt', 'w')\n",
    "\n",
    "whitelist = dict(sorted(paisa_distribution_fixed.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for k, v in whitelist.items():\n",
    "    file_output.write(f'{k}, {v}\\n')\n",
    "\n",
    "file_output.close()\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27599c48-84e7-4048-87a8-0424100c15cf",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eed53d26-0d2e-494f-8316-9aa03d6f2540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading whitelist from file\n",
    "file = open('Corpus/whitelist.txt')\n",
    "whitelist = [word.split(', ')[0] for word in file]\n",
    "file.close()\n",
    "\n",
    "dictionary = {key: value+1 for value, key in enumerate(whitelist)}\n",
    "inverted_dictionary = {key+1: value for key, value in enumerate(whitelist)}\n",
    "\n",
    "# Loading stopwords list from file\n",
    "file = open('Corpus/stopwords.txt')\n",
    "stopwords = [word[:-1] for word in file]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d830c54-e9ee-40fd-a4ec-a809f0b07ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "4500000\n",
      "5000000\n",
      "5500000\n",
      "6000000\n",
      "6500000\n",
      "7000000\n",
      "7500000\n",
      "8000000\n"
     ]
    }
   ],
   "source": [
    "# Paisa tokenization\n",
    "file = open('Corpus/paisa.raw.utf8')\n",
    "file_out = open('Corpus/paisa_tokenization.txt', 'w')\n",
    "\n",
    "# for each line\n",
    "for i, line in enumerate(file):\n",
    "    if line[0] != '#' and line[0] != '<' and line != '\\n':\n",
    "\n",
    "        tokens = tokenize(line)\n",
    "\n",
    "        # filter tokens and save result on disk\n",
    "        for token in tokens:\n",
    "            if token not in stopwords:\n",
    "                if token in dictionary:\n",
    "                    token_index = dictionary[token]\n",
    "                    file_out.write(f'{token_index} ')\n",
    "                else:\n",
    "                    #print(token)\n",
    "                    file_out.write('0 ')\n",
    "\n",
    "        file_out.write('\\n')\n",
    "    \n",
    "    # Update output\n",
    "    if i % 500000 == 0:\n",
    "        print(i)\n",
    "\n",
    "file.close()\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06a11642-1c1a-495c-9b88-eb37c1c14847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_file(input_file_path, output_file_path, update_rate):\n",
    "    file = open(input_file_path)\n",
    "    file_out = open(output_file_path, 'w')\n",
    "\n",
    "    # for each line\n",
    "    for i, line in enumerate(file):\n",
    "        tokens = tokenize(line)\n",
    "\n",
    "        # filter tokens and save result on disk\n",
    "        for token in tokens:\n",
    "            if token not in stopwords:\n",
    "                if token in dictionary:\n",
    "                    token_index = dictionary[token]\n",
    "                    file_out.write(f'{token_index} ')\n",
    "                else:\n",
    "                    #print(token)\n",
    "                    file_out.write('0 ')\n",
    "\n",
    "        file_out.write('\\n')\n",
    "\n",
    "        # Update output\n",
    "        if i % update_rate == 0:\n",
    "            print(i)\n",
    "\n",
    "    file.close()\n",
    "    file_out.close()\n",
    "    \n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a4f7248-d9d6-42b0-9cab-8df5c8596fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "Done.\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "Done.\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Done.\n",
      "0\n",
      "250\n",
      "500\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Smaller Corpuses tokenization\n",
    "tokenize_file('Corpus/corpuscollocazioni.txt', 'Corpus/collocazioni_tokenization.txt', 50000)\n",
    "tokenize_file('Corpus/polirematics.txt', 'Corpus/polirematics_tokenization.txt', 10000)\n",
    "tokenize_file('Corpus/sayings_corpus.txt', 'Corpus/sayings_tokenization.txt', 1000)\n",
    "tokenize_file('Corpus/songs_corpus.txt', 'Corpus/songs_tokenization.txt', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b137423-fbc6-4197-b135-0dfdbcb71a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
